# Copy to .env and fill in the values you have

# Select provider: azure-openai (default) or azure-foundry
AI_PROVIDER=azure-openai

# Azure OpenAI (for OpenAI family models via Azure OpenAI Service)
AZURE_OPENAI_ENDPOINT=https://<your-resource-name>.openai.azure.com
AZURE_OPENAI_API_KEY=xxxxx
AZURE_OPENAI_API_VERSION=2024-02-01

# Azure AI Foundry Inference (for third-party models like Grok, Mistral, Llama)
# You must create a project connection key in Azure AI Foundry
AZURE_INFERENCE_ENDPOINT=https://models.inference.ai.azure.com
AZURE_INFERENCE_API_KEY=xxxxx
# Optional API version (if required by your endpoint)
# AZURE_INFERENCE_API_VERSION=2024-12-01-preview

# Models
# Renamed to provider-agnostic terms: primary/secondary
# For azure-openai, these are the deployment names you created in your Azure OpenAI resource
# For azure-foundry, provide model IDs, e.g., "grok-2", "mistral-large", "gpt-4o-mini"
AZURE_PRIMARY_DEPLOYMENT=gpt-4o
AZURE_SECONDARY_DEPLOYMENT=gpt-4o-mini
GROK_MODEL=grok-2

# Example: switch to Foundry + Grok for primary analysis
# AI_PROVIDER=azure-foundry
# PRIMARY_MODEL=grok-2
# SECONDARY_MODEL=gpt-4o-mini
